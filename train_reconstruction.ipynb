{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4658d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_reconstruction.py --dataset=sbm_200 --k=16 --smallest=True --diffusion_model=3dn0ayuq --generator_layers=4 --generator_data_channels=64 --batch_size=8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: l_cosmo. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.16.2\n",
      "wandb: Run data is saved locally in ./wandb/run-20240201_211828-kjc2xht0\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run self-cross-hugg_k-16_sm-True_dm-3dn0ayuq\n",
      "wandb: ⭐️ View project at https://wandb.ai/l_cosmo/graph_diffusion_recon\n",
      "wandb: 🚀 View run at https://wandb.ai/l_cosmo/graph_diffusion_recon/runs/kjc2xht0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point dim 16\n",
      "Tot #200\n",
      ">>>>  torch.Size([160, 187, 16])\n",
      "compute graphs statistics\n",
      "computing degree:  tensor(0.0008)\n",
      "computing cluster:  tensor(0.0315)\n",
      "computing spectral:  tensor(0.0026)\n",
      "Point dim 16\n",
      "Tot #200\n",
      ">>>>  torch.Size([160, 187, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | generator | PPGNGenerator     | 179 K \n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "179 K     Trainable params\n",
      "0         Non-trainable params\n",
      "179 K     Total params\n",
      "0.716     Total estimated model params size (MB)\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:489: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]\r",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\r",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\r",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 852.15it/s]\r",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 363.49it/s]\r",
      "                                                                            \r",
      "\r",
      "Training: 0it [00:00, ?it/s]\r",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\r",
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: 🚀 View run self-cross-hugg_k-16_sm-True_dm-3dn0ayuq at: https://wandb.ai/l_cosmo/graph_diffusion_recon/runs/kjc2xht0\n",
      "wandb: ️⚡ View job at https://wandb.ai/l_cosmo/graph_diffusion_recon/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNjExNjA2Ng==/version_details/v0\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20240201_211828-kjc2xht0/logs\n",
      "Traceback (most recent call last):\n",
      "  File \"train_reconstruction.py\", line 167, in <module>\n",
      "    trainer.fit(ref, train_dataloader, val_dataloader)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\n",
      "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 203, in advance\n",
      "    batch_output = self.batch_loop.run(kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 89, in advance\n",
      "    outputs = self.manual_loop.run(kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 110, in advance\n",
      "    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1704, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 358, in training_step\n",
      "    return self.model.training_step(*args, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/reconstruction.py\", line 85, in training_step\n",
      "    fake_adj, fake_node_features, fake_edge_features = generator(noise, noisy_real_eigval, noisy_real_eigvec, mask_real)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn_gan.py\", line 160, in forward\n",
      "    adj = self.powerful(adj, x, mask)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn.py\", line 134, in forward\n",
      "    u = conv(u, mask) + (u if self.residual else 0)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn.py\", line 32, in forward\n",
      "    out = out1 @ out2                                      # batch, out_feat, N, N\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 31.74 GiB total capacity; 6.18 GiB already allocated; 41.12 MiB free; 6.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/threading.py\", line 870, in run\n",
      "    self.run()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 286, in check_stop_status\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 268, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
      "    self._loop_check_status(\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 784, in deliver_stop_status\n",
      "    local_handle = request()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 792, in deliver_network_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 484, in _deliver_stop_status\n",
      "    return self._deliver_network_status(status)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 500, in _deliver_network_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
      "    return self._deliver_record(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    interface._publish(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._send_message(msg)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train_reconstruction.py --dataset=sbm_200 --k=16 --smallest=False --diffusion_model=3dn0ayuq --generator_layers=4 --generator_data_channels=64 --batch_size=8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: l_cosmo. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.16.2\n",
      "wandb: Run data is saved locally in ./wandb/run-20240201_211909-u4hfnotg\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run self-cross-hugg_k-16_sm-False_dm-3dn0ayuq\n",
      "wandb: ⭐️ View project at https://wandb.ai/l_cosmo/graph_diffusion_recon\n",
      "wandb: 🚀 View run at https://wandb.ai/l_cosmo/graph_diffusion_recon/runs/u4hfnotg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point dim 16\n",
      "Tot #200\n",
      ">>>>  torch.Size([160, 187, 16])\n",
      "compute graphs statistics\n",
      "computing degree:  tensor(0.0008)\n",
      "computing cluster:  tensor(0.0315)\n",
      "computing spectral:  tensor(0.0026)\n",
      "Point dim 16\n",
      "Tot #200\n",
      ">>>>  torch.Size([160, 187, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | generator | PPGNGenerator     | 179 K \n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "179 K     Trainable params\n",
      "0         Non-trainable params\n",
      "179 K     Total params\n",
      "0.716     Total estimated model params size (MB)\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:489: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]\r",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\r",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\r",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 755.73it/s]\r",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 96.94it/s] \r",
      "                                                                           \r",
      "\r",
      "Training: 0it [00:00, ?it/s]\r",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\r",
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: 🚀 View run self-cross-hugg_k-16_sm-False_dm-3dn0ayuq at: https://wandb.ai/l_cosmo/graph_diffusion_recon/runs/u4hfnotg\n",
      "wandb: ️⚡ View job at https://wandb.ai/l_cosmo/graph_diffusion_recon/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNjExNjA2Ng==/version_details/v0\n",
      "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20240201_211909-u4hfnotg/logs\n",
      "Traceback (most recent call last):\n",
      "  File \"train_reconstruction.py\", line 167, in <module>\n",
      "    trainer.fit(ref, train_dataloader, val_dataloader)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\n",
      "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 203, in advance\n",
      "    batch_output = self.batch_loop.run(kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 89, in advance\n",
      "    outputs = self.manual_loop.run(kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 110, in advance\n",
      "    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1704, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 358, in training_step\n",
      "    return self.model.training_step(*args, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/reconstruction.py\", line 85, in training_step\n",
      "    fake_adj, fake_node_features, fake_edge_features = generator(noise, noisy_real_eigval, noisy_real_eigvec, mask_real)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn_gan.py\", line 160, in forward\n",
      "    adj = self.powerful(adj, x, mask)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn.py\", line 134, in forward\n",
      "    u = conv(u, mask) + (u if self.residual else 0)\n",
      "  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/lcosmo/PROJECTS/Spectral-Graph-Diffusion/models/ppgn.py\", line 32, in forward\n",
      "    out = out1 @ out2                                      # batch, out_feat, N, N\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 31.74 GiB total capacity; 6.18 GiB already allocated; 41.12 MiB free; 6.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"WANDB_MODE\"]=\"online\"\n",
    "\n",
    "diffusion_models = ['ytcfkbrz','2shqiia7'] #sbm\n",
    "diffusion_models = ['3dn0ayuq']\n",
    "rec_weights = [1e0]\n",
    "# # GAN arguments\n",
    "# parser.add_argument('--generator_layers', type=int, default=8)\n",
    "# parser.add_argument('--generator_data_channels', type=int, default=32)\n",
    "# parser.add_argument('--discriminator_layers', type=int, default=64)\n",
    "# parser.add_argument('--discriminator_data_channels', type=int, default=32)\n",
    "# parser.add_argument('--rec_weight', type=float, default=1e-1)\n",
    "\n",
    "\n",
    "combo = [[4, 64, 4, 16]]\n",
    "\n",
    "# combo = [[6, 64, 4, 32]]\n",
    "ks = [16]\n",
    "sms = [True,False]\n",
    "params = list(product(diffusion_models,rec_weights,combo,ks,sms))\n",
    "# random.shuffle(params)\n",
    "params\n",
    "dataset = 'sbm_200'\n",
    "batch_size = 8\n",
    "\n",
    "for diffusion_model, rec_weight, (generator_layers,generator_data_channels,discriminator_layers,discriminator_data_channels), k, sm in params:\n",
    "    command = f'python train_reconstruction.py --dataset={dataset} --k={k} --smallest={sm} --diffusion_model={diffusion_model} --generator_layers={generator_layers} --generator_data_channels={generator_data_channels} --batch_size={batch_size} '\n",
    "    print(command)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db68c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502d8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import argparse\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils.misc import *\n",
    "from utils.utils_score import *\n",
    "from dataset.load_data_generated import *\n",
    "\n",
    "from dataset.load_data_generated import *\n",
    "import sklearn.preprocessing \n",
    "\n",
    "from models.transformer import Transformer\n",
    "\n",
    "from models.reconstruction import Refiner2\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from train_refiner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090df7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(100).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4265fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_arg_parser().parse_args(['--diffusion_model','6mul37q1'])\n",
    "\n",
    "args.dataset='qm9'\n",
    "args.k=9\n",
    "args.smallest=False\n",
    "#################### load diffusion_model ##############################\n",
    "# diffusion_model_path = glob.glob(f'graph_diffusion_perceptron/{args.diffusion_model}/checkpoints/epoch*.ckpt')[0]\n",
    "# model = Transformer.load_from_checkpoint(diffusion_model_path)\n",
    "\n",
    "# model.hparams.update(args.__dict__)\n",
    "# args = model.hparams\n",
    "# args.qm9 = args.dataset[:3] in [\"qm9\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da3f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d882ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point dim 9\n",
      "Tot #130828\n",
      ">>>>  torch.Size([104663, 9, 13])\n",
      "compute graphs statistics\n",
      "computing degree:  tensor(1.3351e-05)\n",
      "computing cluster:  tensor(0.0002)\n",
      "computing spectral:  tensor(5.7340e-05)\n",
      "Point dim 9\n",
      "Tot #130828\n",
      ">>>>  torch.Size([104663, 9, 13])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################### load real graphs training set ######################\n",
    "graphs_train_set = LaplacianDatasetNX(args.dataset,'data/'+args.dataset,point_dim=args.k, smallest=args.smallest, split='train', nodefeatures=args.dataset[:3] in [\"qm9\"])\n",
    "graphs_test_set = LaplacianDatasetNX(args.dataset,'data/'+args.dataset,point_dim=args.k, smallest=args.smallest, split='test', nodefeatures=args.dataset[:3] in [\"qm9\"])\n",
    "\n",
    "graphs_train_set.get_extra_data(False)\n",
    "\n",
    "graphs_train_set.get_extra_data()\n",
    "real_eval = torch.stack([t[1] for t in graphs_train_set],0)\n",
    "real_evec = torch.stack([t[0] for t in graphs_train_set],0)\n",
    "real_adj = torch.stack([t[-1][0] for t in graphs_train_set],0)\n",
    "\n",
    "real_emask = torch.stack([t[3] for t in graphs_train_set],0)\n",
    "real_edge_features = torch.stack([t[4] for t in graphs_train_set],0)\n",
    "\n",
    "real_evec,real_eval = graphs_train_set.unscale_xy(real_evec,real_eval)\n",
    "real_evec *= real_emask[:,None,:] \n",
    "real_eval *= real_emask           \n",
    "\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(real_evec,real_eval,real_adj,real_edge_features)\n",
    "train_dataloader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0,pin_memory=True)\n",
    "    \n",
    "################### load real graphs training set ######################\n",
    "graphs_test_set.get_extra_data()\n",
    "real_eval = torch.stack([t[1] for t in graphs_test_set],0)\n",
    "real_evec = torch.stack([t[0] for t in graphs_test_set],0)\n",
    "real_adj = torch.stack([t[-1][0] for t in graphs_test_set],0)\n",
    "\n",
    "real_emask = torch.stack([t[3] for t in graphs_test_set],0)\n",
    "real_edge_features = torch.stack([t[4] for t in graphs_test_set],0)\n",
    "\n",
    "real_evec,real_eval = graphs_train_set.unscale_xy(real_evec,real_eval)\n",
    "real_evec *= real_emask[:,None,:] \n",
    "real_eval *= real_emask           \n",
    "real_eval = real_eval[:,None]\n",
    "\n",
    "val_set = torch.utils.data.TensorDataset(real_evec,real_eval,real_adj,real_edge_features)\n",
    "val_dataloader = DataLoader(ConcatDataset(graphs_test_set,val_set), batch_size=16, shuffle=True, num_workers=0,pin_memory=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0134e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.0000e+00,  1.4809e-01, -2.0727e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00, -5.3254e-01,  5.9682e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  4.1025e-01, -3.2439e-16,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00, -2.2470e-01,  9.0206e-17,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  2.9168e-01, -3.1756e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00, -2.7193e-11,  2.4225e-11,  ...,  1.1254e-01,\n",
       "            1.5556e-01,  2.5330e-03]],\n",
       " \n",
       "         [[-0.0000e+00,  1.6026e-01, -1.0372e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00, -7.0156e-01,  4.2146e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  3.2332e-01, -1.9019e-02,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  1.6280e-01,  5.1718e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00, -3.7191e-02, -1.2728e-01,  ..., -7.4506e-09,\n",
       "            1.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00, -2.7193e-11,  2.4225e-11,  ...,  1.1254e-01,\n",
       "            1.5556e-01,  2.5330e-03]],\n",
       " \n",
       "         [[ 1.0774e-01,  1.7640e-01, -5.9686e-02,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-4.7560e-01, -7.1165e-01,  1.3849e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.8041e-16,  2.3171e-01, -6.0638e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 1.5237e-01, -1.1464e-01, -1.1022e-01,  ..., -7.4506e-09,\n",
       "            1.0000e+00,  0.0000e+00],\n",
       "          [ 1.7086e-01, -2.5770e-01, -2.6793e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 8.9258e-02,  3.1946e-01,  9.8028e-02,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.6166e-01,  7.1088e-02,  2.4570e-01,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 5.9999e-01, -2.3942e-01, -5.1452e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-6.5433e-01, -1.5655e-01, -3.4345e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [-2.1079e-01,  4.1298e-01,  1.4616e-01,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.6796e-01,  7.5591e-02,  5.5704e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-7.2200e-02, -2.2444e-02, -2.6600e-01,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-4.2960e-02, -7.8063e-17, -1.2292e-01,  ..., -7.4506e-09,\n",
       "            1.0000e+00,  0.0000e+00],\n",
       "          [ 1.5107e-01,  2.6715e-16,  3.1375e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-3.3723e-01, -5.6725e-16, -3.6414e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [-1.6633e-01, -2.3072e-16,  5.4926e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.0929e-01, -2.7060e-01, -4.2634e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-3.6036e-01,  6.5328e-01,  1.1259e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-7.0634e-02,  1.1912e-01,  3.5428e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.4885e-01, -3.5215e-01, -5.2614e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-5.5724e-01,  5.6979e-01, -9.9060e-02,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 1.5817e-01,  1.9274e-01,  6.6703e-02,  ...,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.4885e-01,  3.5215e-01, -5.2614e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-7.0634e-02, -1.1912e-01,  3.5428e-01,  ..., -7.4506e-09,\n",
       "            0.0000e+00,  0.0000e+00]]]),\n",
       " tensor([[[ 0.0000e+00,  4.5962e+00,  3.8794e+00,  3.1826e+00,  1.6527e+00,\n",
       "            1.4843e+00,  7.3692e-01,  4.6791e-01,  3.6047e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  5.3775e+00,  5.0634e+00,  3.2380e+00,  2.8414e+00,\n",
       "            1.8644e+00,  1.0374e+00,  5.7788e-01,  2.1648e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.4142e+00,  5.0344e+00,  3.3204e+00,  3.0000e+00,  2.5858e+00,\n",
       "            1.1945e+00,  1.0000e+00,  4.5074e-01,  2.6970e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3314e+00,  4.1324e+00,  3.0000e+00,  2.6501e+00,  1.0000e+00,\n",
       "            1.0000e+00,  6.4842e-01,  2.3773e-01,  1.2842e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.7625e+00,  3.8794e+00,  3.0000e+00,  2.4366e+00,  1.6527e+00,\n",
       "            1.5168e+00,  4.6791e-01,  2.8406e-01,  1.6184e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.1743e+00,  3.4670e+00,  3.0000e+00,  2.4961e+00,  2.1609e+00,\n",
       "            1.0000e+00,  5.2423e-01,  1.7742e-01,  9.5114e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.2595e+00,  4.4142e+00,  3.6008e+00,  2.4743e+00,  1.6250e+00,\n",
       "            1.5858e+00,  7.0531e-01,  3.3516e-01,  1.5956e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.9396e+00,  4.4490e+00,  3.7179e+00,  3.0000e+00,  2.4148e+00,\n",
       "            2.0000e+00,  9.1157e-01,  5.6713e-01, -2.6565e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8748e+00,  4.2564e+00,  3.5818e+00,  2.4586e+00,  2.0000e+00,\n",
       "            1.5205e+00,  8.5116e-01,  4.5670e-01, -2.6450e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.4942e+00,  3.7130e+00,  3.5078e+00,  2.6056e+00,  2.1317e+00,\n",
       "            1.2034e+00,  8.1005e-01,  5.3424e-01, -6.1838e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.6714e+00,  4.6819e+00,  3.3738e+00,  2.3978e+00,  1.7030e+00,\n",
       "            1.1275e+00,  7.0322e-01,  3.4132e-01,  1.1221e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.6996e+00,  4.2631e+00,  3.7609e+00,  3.5157e+00,  3.0000e+00,\n",
       "            1.8174e+00,  1.5395e+00,  4.0385e-01, -4.6071e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.5231e+00,  3.9563e+00,  2.4851e+00,  2.2091e+00,  1.4331e+00,\n",
       "            6.6174e-01,  5.5871e-01,  1.7291e-01,  4.4333e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3746e+00,  3.9083e+00,  3.0821e+00,  2.0000e+00,  1.5285e+00,\n",
       "            1.2484e+00,  6.2501e-01,  2.3313e-01, -7.3482e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.7321e+00,  3.7321e+00,  3.0000e+00,  3.0000e+00,  1.2679e+00,\n",
       "            1.0000e+00,  1.0000e+00,  2.6795e-01,  1.2971e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.4605e+00,  3.2470e+00,  3.0000e+00,  2.2391e+00,  1.5550e+00,\n",
       "            1.0000e+00,  3.0037e-01,  1.9806e-01,  2.3281e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.5879e+00,  4.0535e+00,  3.5685e+00,  3.0000e+00,  3.0000e+00,\n",
       "            1.7081e+00,  7.7268e-01,  3.0934e-01, -1.0379e-15,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.2332e+00,  3.5643e+00,  3.0000e+00,  2.6729e+00,\n",
       "            1.6353e+00,  7.2760e-01,  1.6672e-01, -7.8397e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8747e+00,  3.9627e+00,  3.4405e+00,  2.1373e+00,  1.5968e+00,\n",
       "            1.0000e+00,  7.0767e-01,  2.8036e-01, -2.0123e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.0935e+00,  3.6065e+00,  3.2395e+00,  2.2627e+00,  2.0000e+00,\n",
       "            1.0814e+00,  4.2675e-01,  2.8967e-01, -3.3529e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.1451e+00,  3.6180e+00,  3.5240e+00,  3.0000e+00,  2.0000e+00,\n",
       "            1.3820e+00,  1.0000e+00,  3.3092e-01, -5.7956e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.4763e+00,  3.8408e+00,  3.0000e+00,  2.3537e+00,\n",
       "            1.4977e+00,  6.1772e-01,  2.1368e-01, -5.8811e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  5.5181e+00,  4.2748e+00,  2.9418e+00,  2.3148e+00,\n",
       "            1.4629e+00,  8.3584e-01,  6.5167e-01, -4.2021e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.3429e+00,  3.4142e+00,  2.4707e+00,  2.0000e+00,\n",
       "            1.0000e+00,  5.8579e-01,  1.8639e-01, -2.1197e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.7619e+00,  3.9280e+00,  3.2711e+00,  2.6180e+00,  1.7289e+00,\n",
       "            1.0720e+00,  3.8197e-01,  2.3810e-01, -1.3956e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  5.1762e+00,  4.2659e+00,  2.4815e+00,  1.6280e+00,\n",
       "            1.0000e+00,  1.0000e+00,  4.4840e-01, -1.8837e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8070e+00,  4.0769e+00,  3.4142e+00,  2.2110e+00,  1.4857e+00,\n",
       "            1.0000e+00,  5.8579e-01,  4.1941e-01, -2.1050e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.0159e+00,  3.9119e+00,  3.0000e+00,  3.0000e+00,  2.2734e+00,\n",
       "            2.0000e+00,  5.5746e-01,  2.4128e-01,  4.0463e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.5719e+00,  4.0493e+00,  3.4142e+00,  2.7972e+00,  1.6652e+00,\n",
       "            1.2067e+00,  7.0970e-01,  5.8579e-01, -3.9012e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.9612e+00,  4.7321e+00,  3.7899e+00,  3.0000e+00,  2.2987e+00,\n",
       "            1.6222e+00,  1.2679e+00,  3.2805e-01, -1.8646e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.4495e+00,  4.8136e+00,  3.0000e+00,  3.0000e+00,  2.5293e+00,\n",
       "            2.0000e+00,  6.5708e-01,  5.5051e-01, -1.7785e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.6993e+00,  4.4485e+00,  3.7031e+00,  3.0000e+00,  2.5278e+00,\n",
       "            1.3125e+00,  8.0866e-01,  5.0023e-01, -2.9366e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.6855e+00,  3.4142e+00,  2.3349e+00,  2.0000e+00,\n",
       "            7.2867e-01,  5.8579e-01,  2.5088e-01,  6.4625e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8472e+00,  4.2254e+00,  2.8533e+00,  2.4293e+00,  1.8361e+00,\n",
       "            8.5220e-01,  6.3975e-01,  3.1664e-01, -1.9217e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.4770e+00,  4.3686e+00,  3.1821e+00,  2.5450e+00,  1.6888e+00,\n",
       "            1.0838e+00,  4.4569e-01,  2.0895e-01,  5.3301e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8538e+00,  4.1332e+00,  3.8794e+00,  3.0000e+00,  1.6527e+00,\n",
       "            1.4136e+00,  5.9948e-01,  4.6791e-01, -3.5195e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.7195e+00,  3.8118e+00,  3.0000e+00,  3.0000e+00,  2.0000e+00,\n",
       "            1.4471e+00,  7.7679e-01,  2.4483e-01, -2.5910e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.5813e+00,  3.6583e+00,  3.0000e+00,  2.4079e+00,  2.0000e+00,\n",
       "            1.5347e+00,  6.7408e-01,  1.4372e-01, -4.2505e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  6.1249e+00,  5.2361e+00,  3.4142e+00,  2.6367e+00,\n",
       "            1.2384e+00,  7.6393e-01,  5.8579e-01, -7.3024e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.1782e+00,  4.3762e+00,  3.0000e+00,  2.7405e+00,  2.1614e+00,\n",
       "            1.4655e+00,  7.8768e-01,  2.9041e-01, -1.7964e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.7996e+00,  4.8107e+00,  3.8297e+00,  3.0000e+00,  2.5794e+00,\n",
       "            2.1847e+00,  1.4061e+00,  3.8977e-01,  1.7796e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.6071e+00,  3.9331e+00,  3.4635e+00,  2.7893e+00,  1.7523e+00,\n",
       "            1.1879e+00,  7.5842e-01,  5.0841e-01,  5.6803e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.0124e+00,  4.1360e+00,  3.6061e+00,  2.7189e+00,  2.3056e+00,\n",
       "            1.1249e+00,  8.5701e-01,  2.3904e-01, -2.8268e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8794e+00,  4.4605e+00,  3.0000e+00,  2.6527e+00,  2.2391e+00,\n",
       "            1.4679e+00,  1.0000e+00,  3.0037e-01,  2.8958e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3574e+00,  4.4142e+00,  3.7056e+00,  3.0000e+00,  2.1619e+00,\n",
       "            1.5858e+00,  1.2859e+00,  4.8924e-01, -7.7985e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.5616e+00,  4.3028e+00,  3.0000e+00,  2.6180e+00,  2.0000e+00,\n",
       "            1.4384e+00,  6.9722e-01,  3.8197e-01, -4.1002e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3818e+00,  3.6180e+00,  3.4609e+00,  2.0960e+00,  1.3820e+00,\n",
       "            1.0000e+00,  7.5662e-01,  3.0469e-01,  1.5542e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.7110e+00,  3.8349e+00,  2.8407e+00,  2.0000e+00,\n",
       "            1.5498e+00,  7.0886e-01,  3.5474e-01, -5.5060e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.7625e+00,  3.8794e+00,  3.0000e+00,  2.4366e+00,  1.6527e+00,\n",
       "            1.5168e+00,  4.6791e-01,  2.8406e-01,  1.6184e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.9122e+00,  4.6996e+00,  3.4142e+00,  2.7609e+00,  2.2865e+00,\n",
       "            8.0131e-01,  5.8579e-01,  5.3950e-01, -4.1960e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3818e+00,  3.4609e+00,  3.0000e+00,  2.6180e+00,  2.0960e+00,\n",
       "            7.5662e-01,  3.8197e-01,  3.0469e-01,  1.6444e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.6855e+00,  3.4142e+00,  2.3349e+00,  2.0000e+00,\n",
       "            7.2867e-01,  5.8579e-01,  2.5088e-01,  8.3834e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.1969e+00,  4.0170e+00,  3.0000e+00,  2.4425e+00,  1.5989e+00,\n",
       "            1.0000e+00,  5.4046e-01,  2.0426e-01, -2.4253e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8468e+00,  3.8525e+00,  2.7338e+00,  2.0782e+00,  1.0000e+00,\n",
       "            7.2229e-01,  5.5459e-01,  2.1179e-01, -6.8104e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.8662e+00,  4.8136e+00,  3.0000e+00,  2.5293e+00,  2.0000e+00,\n",
       "            1.7892e+00,  6.5708e-01,  3.4456e-01,  4.3906e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.6163e+00,  5.3219e+00,  3.6361e+00,  3.0000e+00,  3.0000e+00,\n",
       "            1.8946e+00,  1.0416e+00,  4.8957e-01, -8.6470e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  4.4142e+00,  3.6180e+00,  2.6180e+00,\n",
       "            1.5858e+00,  1.3820e+00,  3.8197e-01, -2.2826e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  4.6412e+00,  3.4142e+00,  2.7237e+00,  1.4108e+00,\n",
       "            1.0000e+00,  5.8579e-01,  2.2429e-01,  2.0001e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.5719e+00,  4.0493e+00,  3.4142e+00,  2.7972e+00,  1.6652e+00,\n",
       "            1.2067e+00,  7.0970e-01,  5.8579e-01, -5.2081e-17,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 5.3314e+00,  4.1324e+00,  3.0000e+00,  2.6501e+00,  2.0000e+00,\n",
       "            2.0000e+00,  6.4842e-01,  2.3773e-01, -2.8465e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  5.5450e+00,  4.8136e+00,  3.4394e+00,  2.5293e+00,\n",
       "            2.1698e+00,  8.4579e-01,  6.5708e-01, -3.1434e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.7113e+00,  4.3680e+00,  3.0941e+00,  2.1841e+00,  1.6247e+00,\n",
       "            1.1632e+00,  5.3405e-01,  3.2062e-01, -1.0328e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.5166e+00,  4.4142e+00,  3.5524e+00,  3.0000e+00,  1.7127e+00,\n",
       "            1.5858e+00,  1.0000e+00,  2.1835e-01, -1.4179e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.5231e+00,  3.9563e+00,  2.4851e+00,  2.2091e+00,  1.4331e+00,\n",
       "            6.6174e-01,  5.5871e-01,  1.7291e-01,  4.4333e-16,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00]]]),\n",
       " tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 1., 0.],\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 0., 1.],\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "          [0., 1., 0.,  ..., 0., 1., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 1., 0., 0.],\n",
       "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "          ...,\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.]]]),\n",
       " tensor([[[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           ...,\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 1.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 1.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [0., 0., 0.]]]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(torch.utils.data.Subset(val_set,range(min(2048,len(val_set)))), batch_size=64, shuffle=False, num_workers=0)\n",
    "next(iter(dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f39339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2200e2b6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader(graphs_test_set[:200], batch_size=16, shuffle=True, num_workers=0,pin_memory=True)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61769fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 133, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1102, in _run\n    self.strategy.setup_environment()\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 130, in setup_environment\n    self.accelerator.setup_environment(self.root_device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py\", line 43, in setup_environment\n    torch.cuda.set_device(root_device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 314, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 41\u001b[0m\n\u001b[1;32m     27\u001b[0m args\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer\u001b[38;5;241m.\u001b[39mfrom_argparse_args(\n\u001b[1;32m     29\u001b[0m     args,\n\u001b[1;32m     30\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     auto_lr_find\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:648\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:107\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     process_args \u001b[38;5;241m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 107\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapping_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 133, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1102, in _run\n    self.strategy.setup_environment()\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 130, in setup_environment\n    self.accelerator.setup_environment(self.root_device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/pytorch_lightning/accelerators/cuda.py\", line 43, in setup_environment\n    torch.cuda.set_device(root_device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 314, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/home/lcosmo/anaconda3/envs/graph_diffusion/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 207, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################\n",
    "args.n_max = graphs_train_set.n_max\n",
    "ref = Refiner2(args)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_last=True,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='rec_loss',\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='rec_loss',\n",
    "    min_delta=0,\n",
    "    patience=20000,\n",
    "    verbose=False,\n",
    "    mode='min')\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    name=f\"{args.model_tag}_k-{args.k}_sm-{args.smallest}_dm-{args.diffusion_model}\",\n",
    "    project=\"graph_diffusion_refinement\",\n",
    "    entity=\"l_cosmo\",\n",
    "    offline=True\n",
    ")\n",
    "# wandb_logger=None\n",
    "\n",
    "args.check_val_every_n_epoch = None\n",
    "trainer = pl.Trainer.from_argparse_args(\n",
    "    args,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=len(train_dataloader),\n",
    "    check_val_every_n_epoch = None,\n",
    "    val_check_interval = args.val_check_interval, \n",
    "    max_epochs = args.max_epochs,\n",
    "    auto_scale_batch_size=\"binsearch\",\n",
    "    auto_lr_find=True\n",
    ")\n",
    "\n",
    "trainer.fit(ref, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc0471f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 32])\n",
      "torch.Size([8, 1, 32])\n",
      "torch.Size([8, 1, 32])\n",
      "torch.Size([8, 1, 32])\n",
      "torch.Size([8, 1, 32])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LaplacianDatasetNX' object has no attribute 'degree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PROJECTS/Spectral-Graph-Diffusion/models/reconstruction.py:112\u001b[0m, in \u001b[0;36mRefiner2.validation_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    109\u001b[0m ori_train_set \u001b[38;5;241m=\u001b[39m gen_test_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdatasets[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    110\u001b[0m gen_test_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdatasets[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 112\u001b[0m degree, cluster,  unique, novel, spectral, degree_degrad, cluster_degrad, spectral_degrad, avg_degrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mori_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_test_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpowerful\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_lin\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(degree)\u001b[38;5;241m.\u001b[39mfloat(), on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m,  torch\u001b[38;5;241m.\u001b[39mtensor(cluster)\u001b[38;5;241m.\u001b[39mfloat(), on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PROJECTS/Spectral-Graph-Diffusion/models/reconstruction.py:210\u001b[0m, in \u001b[0;36mRefiner2.evaluate\u001b[0;34m(self, train_set, test_set, device)\u001b[0m\n\u001b[1;32m    207\u001b[0m     unique,novel,_ \u001b[38;5;241m=\u001b[39m eval_fraction_unique_non_isomorphic_valid(graph_pred_list_remove_empty,graph_train_list)\n\u001b[1;32m    208\u001b[0m     spectral \u001b[38;5;241m=\u001b[39m spectral_stats(graph_test_list, graph_pred_list_remove_empty)\n\u001b[0;32m--> 210\u001b[0m degree_degrad \u001b[38;5;241m=\u001b[39m degree\u001b[38;5;241m/\u001b[39m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\n\u001b[1;32m    211\u001b[0m cluster_degrad \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m/\u001b[39mtrain_set\u001b[38;5;241m.\u001b[39mcluster\n\u001b[1;32m    212\u001b[0m spectral_degrad \u001b[38;5;241m=\u001b[39m spectral\u001b[38;5;241m/\u001b[39mtrain_set\u001b[38;5;241m.\u001b[39mspectral\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LaplacianDatasetNX' object has no attribute 'degree'"
     ]
    }
   ],
   "source": [
    "ref.validation_epoch_end([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aea1f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f06c98a3670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref.trainer.val_dataloaders[0].dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:graph_diffusion]",
   "language": "python",
   "name": "conda-env-graph_diffusion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
