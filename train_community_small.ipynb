{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77937260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from utils.dataset import *\n",
    "from utils.misc import *\n",
    "# from utils.data import *\n",
    "# from models.vae_gaussian import *\n",
    "# from models.vae_flow import *\n",
    "# from models.flow import add_spectral_norm, spectral_norm_power_iteration\n",
    "# from evaluation import *\n",
    "\n",
    "\n",
    "from utils.utils_score import *\n",
    "# from diffusionnet import *\n",
    "from dataset.load_data_generated import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ea5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "THOUSAND=1000\n",
    "parser = argparse.ArgumentParser()\n",
    "# Model arguments\n",
    "parser.add_argument('--model', type=str, default='gaussian', choices=['flow', 'gaussian'])\n",
    "parser.add_argument('--num_steps', type=int, default=100)\n",
    "parser.add_argument('--beta_1', type=float, default=1e-4)\n",
    "parser.add_argument('--beta_T', type=float, default=1-1e-4)\n",
    "parser.add_argument('--sched_mode', type=str, default='linear')\n",
    "\n",
    "parser.add_argument('--residual', type=eval, default=True, choices=[True, False])#not_used\n",
    "parser.add_argument('--latent_dim', type=int, default=128)\n",
    "parser.add_argument('--layers', type=int, default=6)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--max_grad_norm', type=float, default=10)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--end_lr', type=float, default=1e-5)\n",
    "parser.add_argument('--sched_start_epoch', type=int, default=5*THOUSAND)\n",
    "parser.add_argument('--sched_end_epoch', type=int, default=20*THOUSAND)\n",
    "parser.add_argument('--max_iters', type=int, default=500*THOUSAND)\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--seed', type=int, default=2020)\n",
    "parser.add_argument('--device', type=str, default=\"cuda\")\n",
    "\n",
    "parser.add_argument('--model_tag', type=str, default='self_cross_v1')\n",
    "parser.add_argument('--dataset', type=str, default='community_12_21_100')\n",
    "parser.add_argument('--k', type=int, default=10)\n",
    "parser.add_argument('--smallest', type=eval, default=False, choices=[True, False])\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.point_dim=args.k\n",
    "args.use_mask=False\n",
    "# seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b288df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point dim 10\n",
      "Comp dimensions...\n",
      "Comp stats...\n",
      "Loaded precomuted eigenquantities\n",
      "Comp Samples...\n",
      "Tot #100\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "# datasetname='community_12_21_100'\n",
    "# datasetname='planar_64_200'\n",
    "# # # datasetname='sbm_200'\n",
    "point_dimns = 10\n",
    "train_set = LaplacianDatasetNX(args.dataset,'data/'+args.dataset,point_dim=args.k, smallest=args.smallest, split='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d14eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=0,pin_memory=True)\n",
    "# valid_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False, num_workers=0,pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb4758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | diffusion | DiffusionPoint | 5.6 M \n",
      "---------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.345    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54eef15362834433a8c37d3b931a5704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'loss' reached 0.64302 (best 0.64302), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=0-step=1.ckpt' as top 1\n",
      "Epoch 1, global step 2: 'loss' reached 0.53699 (best 0.53699), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=1-step=2.ckpt' as top 1\n",
      "Epoch 2, global step 3: 'loss' reached 0.47706 (best 0.47706), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=2-step=3.ckpt' as top 1\n",
      "Epoch 3, global step 4: 'loss' was not in top 1\n",
      "Epoch 4, global step 5: 'loss' was not in top 1\n",
      "Epoch 5, global step 6: 'loss' reached 0.36252 (best 0.36252), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=5-step=6.ckpt' as top 1\n",
      "Epoch 6, global step 7: 'loss' was not in top 1\n",
      "Epoch 7, global step 8: 'loss' was not in top 1\n",
      "Epoch 8, global step 9: 'loss' was not in top 1\n",
      "Epoch 9, global step 10: 'loss' was not in top 1\n",
      "Epoch 10, global step 11: 'loss' was not in top 1\n",
      "Epoch 11, global step 12: 'loss' was not in top 1\n",
      "Epoch 12, global step 13: 'loss' was not in top 1\n",
      "Epoch 13, global step 14: 'loss' was not in top 1\n",
      "Epoch 14, global step 15: 'loss' was not in top 1\n",
      "Epoch 15, global step 16: 'loss' was not in top 1\n",
      "Epoch 16, global step 17: 'loss' was not in top 1\n",
      "Epoch 17, global step 18: 'loss' was not in top 1\n",
      "Epoch 18, global step 19: 'loss' was not in top 1\n",
      "Epoch 19, global step 20: 'loss' was not in top 1\n",
      "Epoch 20, global step 21: 'loss' was not in top 1\n",
      "Epoch 21, global step 22: 'loss' was not in top 1\n",
      "Epoch 22, global step 23: 'loss' was not in top 1\n",
      "Epoch 23, global step 24: 'loss' was not in top 1\n",
      "Epoch 24, global step 25: 'loss' was not in top 1\n",
      "Epoch 25, global step 26: 'loss' was not in top 1\n",
      "Epoch 26, global step 27: 'loss' was not in top 1\n",
      "Epoch 27, global step 28: 'loss' was not in top 1\n",
      "Epoch 28, global step 29: 'loss' was not in top 1\n",
      "Epoch 29, global step 30: 'loss' was not in top 1\n",
      "Epoch 30, global step 31: 'loss' was not in top 1\n",
      "Epoch 31, global step 32: 'loss' was not in top 1\n",
      "Epoch 32, global step 33: 'loss' was not in top 1\n",
      "Epoch 33, global step 34: 'loss' was not in top 1\n",
      "Epoch 34, global step 35: 'loss' was not in top 1\n",
      "Epoch 35, global step 36: 'loss' was not in top 1\n",
      "Epoch 36, global step 37: 'loss' was not in top 1\n",
      "Epoch 37, global step 38: 'loss' was not in top 1\n",
      "Epoch 38, global step 39: 'loss' was not in top 1\n",
      "Epoch 39, global step 40: 'loss' was not in top 1\n",
      "Epoch 40, global step 41: 'loss' was not in top 1\n",
      "Epoch 41, global step 42: 'loss' was not in top 1\n",
      "Epoch 42, global step 43: 'loss' was not in top 1\n",
      "Epoch 43, global step 44: 'loss' was not in top 1\n",
      "Epoch 44, global step 45: 'loss' was not in top 1\n",
      "Epoch 45, global step 46: 'loss' was not in top 1\n",
      "Epoch 46, global step 47: 'loss' was not in top 1\n",
      "Epoch 47, global step 48: 'loss' was not in top 1\n",
      "Epoch 48, global step 49: 'loss' was not in top 1\n",
      "Epoch 49, global step 50: 'loss' reached 0.32327 (best 0.32327), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=49-step=50.ckpt' as top 1\n",
      "Epoch 50, global step 51: 'loss' was not in top 1\n",
      "Epoch 51, global step 52: 'loss' was not in top 1\n",
      "Epoch 52, global step 53: 'loss' was not in top 1\n",
      "Epoch 53, global step 54: 'loss' was not in top 1\n",
      "Epoch 54, global step 55: 'loss' was not in top 1\n",
      "Epoch 55, global step 56: 'loss' was not in top 1\n",
      "Epoch 56, global step 57: 'loss' was not in top 1\n",
      "Epoch 57, global step 58: 'loss' was not in top 1\n",
      "Epoch 58, global step 59: 'loss' was not in top 1\n",
      "Epoch 59, global step 60: 'loss' was not in top 1\n",
      "Epoch 60, global step 61: 'loss' was not in top 1\n",
      "Epoch 61, global step 62: 'loss' was not in top 1\n",
      "Epoch 62, global step 63: 'loss' was not in top 1\n",
      "Epoch 63, global step 64: 'loss' was not in top 1\n",
      "Epoch 64, global step 65: 'loss' was not in top 1\n",
      "Epoch 65, global step 66: 'loss' was not in top 1\n",
      "Epoch 66, global step 67: 'loss' was not in top 1\n",
      "Epoch 67, global step 68: 'loss' was not in top 1\n",
      "Epoch 68, global step 69: 'loss' was not in top 1\n",
      "Epoch 69, global step 70: 'loss' was not in top 1\n",
      "Epoch 70, global step 71: 'loss' was not in top 1\n",
      "Epoch 71, global step 72: 'loss' was not in top 1\n",
      "Epoch 72, global step 73: 'loss' was not in top 1\n",
      "Epoch 73, global step 74: 'loss' was not in top 1\n",
      "Epoch 74, global step 75: 'loss' was not in top 1\n",
      "Epoch 75, global step 76: 'loss' was not in top 1\n",
      "Epoch 76, global step 77: 'loss' was not in top 1\n",
      "Epoch 77, global step 78: 'loss' was not in top 1\n",
      "Epoch 78, global step 79: 'loss' reached 0.31025 (best 0.31025), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=78-step=79.ckpt' as top 1\n",
      "Epoch 79, global step 80: 'loss' was not in top 1\n",
      "Epoch 80, global step 81: 'loss' was not in top 1\n",
      "Epoch 81, global step 82: 'loss' was not in top 1\n",
      "Epoch 82, global step 83: 'loss' was not in top 1\n",
      "Epoch 83, global step 84: 'loss' was not in top 1\n",
      "Epoch 84, global step 85: 'loss' was not in top 1\n",
      "Epoch 85, global step 86: 'loss' was not in top 1\n",
      "Epoch 86, global step 87: 'loss' reached 0.28623 (best 0.28623), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=86-step=87.ckpt' as top 1\n",
      "Epoch 87, global step 88: 'loss' was not in top 1\n",
      "Epoch 88, global step 89: 'loss' was not in top 1\n",
      "Epoch 89, global step 90: 'loss' was not in top 1\n",
      "Epoch 90, global step 91: 'loss' was not in top 1\n",
      "Epoch 91, global step 92: 'loss' was not in top 1\n",
      "Epoch 92, global step 93: 'loss' was not in top 1\n",
      "Epoch 93, global step 94: 'loss' was not in top 1\n",
      "Epoch 94, global step 95: 'loss' was not in top 1\n",
      "Epoch 95, global step 96: 'loss' was not in top 1\n",
      "Epoch 96, global step 97: 'loss' was not in top 1\n",
      "Epoch 97, global step 98: 'loss' was not in top 1\n",
      "Epoch 98, global step 99: 'loss' was not in top 1\n",
      "Epoch 99, global step 100: 'loss' was not in top 1\n",
      "Epoch 100, global step 101: 'loss' was not in top 1\n",
      "Epoch 101, global step 102: 'loss' was not in top 1\n",
      "Epoch 102, global step 103: 'loss' was not in top 1\n",
      "Epoch 103, global step 104: 'loss' was not in top 1\n",
      "Epoch 104, global step 105: 'loss' was not in top 1\n",
      "Epoch 105, global step 106: 'loss' was not in top 1\n",
      "Epoch 106, global step 107: 'loss' was not in top 1\n",
      "Epoch 107, global step 108: 'loss' was not in top 1\n",
      "Epoch 108, global step 109: 'loss' was not in top 1\n",
      "Epoch 109, global step 110: 'loss' was not in top 1\n",
      "Epoch 110, global step 111: 'loss' was not in top 1\n",
      "Epoch 111, global step 112: 'loss' was not in top 1\n",
      "Epoch 112, global step 113: 'loss' was not in top 1\n",
      "Epoch 113, global step 114: 'loss' reached 0.25298 (best 0.25298), saving model to '/home/lcosmo/PROJECTS/diffusion-graphs-perceptron/checkpoints/epoch=113-step=114.ckpt' as top 1\n",
      "Epoch 114, global step 115: 'loss' was not in top 1\n",
      "Epoch 115, global step 116: 'loss' was not in top 1\n",
      "Epoch 116, global step 117: 'loss' was not in top 1\n",
      "Epoch 117, global step 118: 'loss' was not in top 1\n",
      "Epoch 118, global step 119: 'loss' was not in top 1\n",
      "Epoch 119, global step 120: 'loss' was not in top 1\n",
      "Epoch 120, global step 121: 'loss' was not in top 1\n",
      "Epoch 121, global step 122: 'loss' was not in top 1\n",
      "Epoch 122, global step 123: 'loss' was not in top 1\n",
      "Epoch 123, global step 124: 'loss' was not in top 1\n",
      "Epoch 124, global step 125: 'loss' was not in top 1\n",
      "Epoch 125, global step 126: 'loss' was not in top 1\n",
      "Epoch 126, global step 127: 'loss' was not in top 1\n",
      "Epoch 127, global step 128: 'loss' was not in top 1\n",
      "Epoch 128, global step 129: 'loss' was not in top 1\n",
      "Epoch 129, global step 130: 'loss' was not in top 1\n",
      "Epoch 130, global step 131: 'loss' was not in top 1\n",
      "Epoch 131, global step 132: 'loss' was not in top 1\n",
      "Epoch 132, global step 133: 'loss' was not in top 1\n",
      "Epoch 133, global step 134: 'loss' was not in top 1\n",
      "Epoch 134, global step 135: 'loss' was not in top 1\n",
      "Epoch 135, global step 136: 'loss' was not in top 1\n",
      "Epoch 136, global step 137: 'loss' was not in top 1\n",
      "Epoch 137, global step 138: 'loss' was not in top 1\n",
      "Epoch 138, global step 139: 'loss' was not in top 1\n",
      "Epoch 139, global step 140: 'loss' was not in top 1\n",
      "Epoch 140, global step 141: 'loss' was not in top 1\n",
      "Epoch 141, global step 142: 'loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "from models.vae_gaussian import *\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "run_params=args\n",
    "\n",
    "model = GaussianVAE(args)\n",
    "\n",
    "# trainer = pl.Trainer(limit_train_batches=100, max_epochs=10,accelerator=\"auto\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_last=True,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# wandb_logger = WandbLogger(\n",
    "#     name=f\"{args.model_tag}_{args.k}\",\n",
    "#     project=\"graph_diffusion\",\n",
    "#     entity=\"l_cosmo\"\n",
    "# )\n",
    "wandb_logger=None\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(\n",
    "    run_params,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=250\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_dataloader.dataset.datasets[0]\n",
    "model.optimizers().param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b874041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # net_version = 'selfcross_v1'\n",
    "# # ckpt_mgr = CheckpointManager('./experiments/'+datasetname+'_'+net_version+'/')\n",
    "# ckpt = torch.load('../diffusion-graphs/experiments/community_12_21_100_selfcross_v1/ckpt_0.000000_390758.pt')\n",
    "# model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10814dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def sample_graphs(max_nodes, num_eigs,  num_graphs=1024, device='cuda'):\n",
    "    # Sample eigenvectors and eigenvalues\n",
    "    gen_pcs = []\n",
    "    with torch.no_grad():\n",
    "        x = model.sample(max_nodes, num_graphs, 1, point_dim=num_eigs, device=device)\n",
    "        samples_EIGVEC = x.detach().cpu()\n",
    "\n",
    "    # reconstruct laplacian matrix\n",
    "    LLLall =[]\n",
    "    for i,X in enumerate(samples_EIGVEC.cpu()):\n",
    "        xx = X[:-1,:]\n",
    "        yy = X[-1:,:]\n",
    "\n",
    "        xx,yy = unscale_xy(xx,yy)\n",
    "        yy=yy.float()\n",
    "\n",
    "        xx_o=xx\n",
    "\n",
    "        #orthogonal projection\n",
    "        if False:\n",
    "            U,s,V = torch.svd(xx)\n",
    "            xx_o = (U@V).float()\n",
    "\n",
    "        #discard samples that did not led to a quasi-orthonormal basis\n",
    "        L = (xx_o*yy)@xx_o.t()  \n",
    "        err = ((xx_o.t())@xx_o-torch.eye(xx.shape[-1])).norm()\n",
    "\n",
    "        LLLall.append((err,L.numpy()))\n",
    "\n",
    "\n",
    "    LLLall = [l[1] for l in sorted(LLLall, key=lambda e:e[0])][:]\n",
    "\n",
    "    #reconstruct graph\n",
    "    graph_pred_list = []\n",
    "    for i,pp in enumerate(range(len(LLLall))):\n",
    "        mynew = np.around(LLLall[pp],2)\n",
    "        np.fill_diagonal(mynew,np.around(np.diag(mynew),0))\n",
    "        mask = np.diag(mynew)>0\n",
    "\n",
    "        siad = len(np.diag(mynew))\n",
    "        adjb = np.zeros((siad,siad))\n",
    "\n",
    "        for jj in range(len(np.diag(mynew))):\n",
    "            if np.diag(mynew)[jj]!=np.diag(mynew)[jj]:\n",
    "                break\n",
    "            diagval = int(np.diag(mynew)[jj])\n",
    "\n",
    "            row = mynew[jj,:]\n",
    "            row[jj]=100\n",
    "            idx =row.argsort()[:diagval]\n",
    "            adjb[jj,idx] =1\n",
    "\n",
    "        Arec = nx.Graph(adjb[mask,:][:,mask])\n",
    "        graph_pred_list.append(Arec) \n",
    "        \n",
    "    return graph_pred_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from utils.eval_helper import degree_stats, clustering_stats, orbit_stats_all, eval_fraction_unique, eval_fraction_unique_non_isomorphic_valid, spectral_stats\n",
    "\n",
    "\n",
    "# train_set = trainer.train_dataloader.dataset\n",
    "# test_set = trainer.val_dataloaders.dataset\n",
    "\n",
    "\n",
    "def evaluate(train_set, test_set):\n",
    "    batch = next(iter(DataLoader(test_set, batch_size=len(test_set), shuffle=False, num_workers=0)))\n",
    "    b,n,d = batch[0].shape\n",
    "    \n",
    "    graph_pred_list = sample_graphs(max_nodes=n, num_eigs=d,  num_graphs=1024, device='cuda')[:b]\n",
    "    \n",
    "    graph_pred_list_remove_empty = [G for G in graph_pred_list if not G.number_of_nodes() == 0]\n",
    "\n",
    "    #compute metrics\n",
    "    graph_test_list = []\n",
    "    for jj in range(len(test_set)):\n",
    "        laplacian_matrix = np.array(test_set[jj][3].cpu())[:test_set[jj][4],:test_set[jj][4]]\n",
    "        Aori = np.copy(laplacian_matrix)\n",
    "        np.fill_diagonal(Aori,0)\n",
    "        Aori= Aori*(-1)\n",
    "        graph_ref_list.append(nx.from_numpy_array(Aori)) \n",
    "\n",
    "    graph_train_list = []\n",
    "    for jj in range(len(train_set)):\n",
    "        laplacian_matrix = np.array(train_set[jj][3].cpu())[:train_set[jj][4],:train_set[jj][4]]\n",
    "        Aori = np.copy(laplacian_matrix)\n",
    "        np.fill_diagonal(Aori,0)\n",
    "        Aori= Aori*(-1)\n",
    "        graph_train_list.append(nx.from_numpy_array(Aori)) \n",
    "\n",
    "\n",
    "    degree, cluster, orbit, unique, novel, spectral = 0,0,0,0,0,0\n",
    "    if len(graph_pred_list_remove_empty)>0:\n",
    "        degree = degree_stats( graph_test_list,graph_pred_list_remove_empty, compute_emd=False)\n",
    "        cluster = clustering_stats( graph_test_list,graph_pred_list_remove_empty, compute_emd=False)\n",
    "        orbit = orbit_stats_all(graph_test_list, graph_pred_list_remove_empty, compute_emd=False)\n",
    "        unique,novel,_ = eval_fraction_unique_non_isomorphic_valid(graph_pred_list_remove_empty,graph_train_list)\n",
    "        spectral = spectral_stats(graph_test_list, graph_pred_list_remove_empty)\n",
    "        \n",
    "    return degree, cluster, orbit, unique, novel, spectral\n",
    "\n",
    "evaluate(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62c26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59360862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "LLLall =[]\n",
    "\n",
    "for i,X in enumerate(samples_EIGVEC.cpu()):\n",
    "    xx = X[:-1,:]\n",
    "    yy = X[-1:,:]\n",
    "    \n",
    "    xx,yy = unscale_xy(xx,yy)\n",
    "    yy=yy.float()\n",
    "    \n",
    "    xx_o=xx\n",
    "    \n",
    "    #orthogonal projection\n",
    "    if False:\n",
    "        U,s,V = torch.svd(xx)\n",
    "        xx_o = (U@V).float()\n",
    "    \n",
    "    #discard samples that did not led to a quasi-orthonormal basis\n",
    "    L = (xx_o*yy)@xx_o.t()  \n",
    "    err = ((xx_o.t())@xx_o-torch.eye(xx.shape[-1])).norm()\n",
    "    if err>300.0:\n",
    "           continue\n",
    "\n",
    "    LLLall.append((err,L.numpy()))\n",
    "    \n",
    "    if i<5:\n",
    "        I =  torch.eye(xx.shape[0],device=xx.device)#.repeat(xx.shape[0],1,1)\n",
    "        A = -(L - L*I)\n",
    "    #     A = A - A.min(-1)[0].min(-1)[0]\n",
    "    #     A = A/A.max(-1)[0].max(-1)[0] \n",
    "    #     A = A - A*I + I\n",
    "        ZZ = (xx_o.t())@xx_o \n",
    "    #     print(ZZ)\n",
    "    #     plt.figure()\n",
    "        plt.imshow(ZZ)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "LLLall = [l[1] for l in sorted(LLLall, key=lambda e:e[0])][:20]\n",
    "print(len(LLLall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# LLLall =[]\n",
    "\n",
    "\n",
    "# point_dimns = 12\n",
    "# dataset = LaplacianDatasetNX(datasetname,datasetname,point_dim=point_dimns, smallest=False)\n",
    "\n",
    "# # for i,X in enumerate(samples_EIGVEC.cpu()):\n",
    "# for i in range(len(dataset)):\n",
    "    \n",
    "#     xx = dataset[i][0].cpu()\n",
    "#     yy = dataset[i][1].cpu().float()\n",
    "#     print(xx.shape)\n",
    "#     U,s,V = torch.svd(xx)\n",
    "#     xx_o = (U@V).float()\n",
    "#     L = (xx_o*yy)@xx_o.t()  \n",
    "#     LLLall.append(L.numpy())\n",
    "    \n",
    "#     if i<5:\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "        \n",
    "#         I =  torch.eye(xx.shape[0],device=xx.device)#.repeat(xx.shape[0],1,1)\n",
    "#         A = -(L - L*I)\n",
    "#         ZZ = (xx.t())@xx \n",
    "#         axes[0].imshow(L)\n",
    "        \n",
    "#         axes[1].imshow(dataset[i][3].cpu())\n",
    "        \n",
    "        \n",
    "# #         plt.colorbar()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c947d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4dbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a51d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ab268",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_pred_list = []\n",
    "\n",
    "for i,pp in enumerate(range(len(LLLall))):\n",
    "    mynew = np.around(LLLall[pp],2)\n",
    "    \n",
    "#     dg = np.diag(mynew)+0\n",
    "    np.fill_diagonal(mynew,np.around(np.diag(mynew),0))\n",
    "    \n",
    "#     np.fill_diagonal(mynew,torch.diag(dataset[i][3]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask = np.diag(mynew)>0\n",
    "\n",
    "    siad = len(np.diag(mynew))\n",
    "    adjb = np.zeros((siad,siad))\n",
    "\n",
    "    for jj in range(len(np.diag(mynew))):\n",
    "        if np.diag(mynew)[jj]!=np.diag(mynew)[jj]:\n",
    "            break\n",
    "        diagval = int(np.diag(mynew)[jj])\n",
    "\n",
    "        row = mynew[jj,:]\n",
    "        row[jj]=100\n",
    "        idx =row.argsort()[:diagval]\n",
    "        adjb[jj,idx] =1\n",
    "\n",
    "    Arec = nx.Graph(adjb[mask,:][:,mask])\n",
    "   \n",
    "    graph_pred_list.append(Arec) \n",
    "    if i< 5:\n",
    "#         print(dg)\n",
    "#         print(mask.sum())\n",
    "        plt.figure()\n",
    "        nx.draw_kamada_kawai(Arec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ref_list = []\n",
    "for jj in range(len(test_set)):\n",
    "    laplacian_matrix = np.array(dataset[jj][3].cpu())[:dataset[jj][4],:dataset[jj][4]]\n",
    "    Aori = np.copy(laplacian_matrix)\n",
    "    np.fill_diagonal(Aori,0)\n",
    "    Aori= Aori*(-1)\n",
    "    graph_ref_list.append(nx.from_numpy_array(Aori)) \n",
    " \n",
    "graph_ref_list_2 = []\n",
    "for jj in range(len(train_set)):\n",
    "    laplacian_matrix = np.array(dataset[jj][3].cpu())[:dataset[jj][4],:dataset[jj][4]]\n",
    "    Aori = np.copy(laplacian_matrix)\n",
    "    np.fill_diagonal(Aori,0)\n",
    "    Aori= Aori*(-1)\n",
    "    graph_ref_list.append(nx.from_numpy_array(Aori)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f891f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graph_ref_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare degree\n",
    "\n",
    "import utils.evaluat_meas as mmd\n",
    "\n",
    "# mmd_degree = degree_stats(graph_test, graphs)\n",
    "sample_ref_deg  = []\n",
    "sample_pred_deg  = []\n",
    "\n",
    "\n",
    "for gr in graph_ref_list:\n",
    "    degree_temp = np.array(nx.degree_histogram(gr))\n",
    "    sample_ref_deg.append(degree_temp)\n",
    "    \n",
    "graph_pred_list_remove_empty = [G for G in graph_pred_list if not G.number_of_nodes() == 0]\n",
    "for gp in graph_pred_list_remove_empty:\n",
    "    degree_temp = np.array(nx.degree_histogram(gp))\n",
    "    sample_pred_deg .append(degree_temp)\n",
    "    \n",
    "mmd_dist_deg = mmd.compute_mmd(sample_ref_deg , sample_pred_deg , kernel=mmd.gaussian_tv)\n",
    "mmd_dist_deg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79844094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_stats\n",
    "sample_ref_clus = []\n",
    "sample_pred_clus = []\n",
    "\n",
    "bins=100\n",
    "for gr in graph_ref_list:\n",
    "    clustering_coeffs_list = list(nx.clustering(gr).values())\n",
    "    hist,_ = np.histogram(clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "    sample_ref_clus.append(hist)\n",
    "    \n",
    "\n",
    "                                  \n",
    "for gp in graph_pred_list_remove_empty:\n",
    "    clustering_coeffs_list =  list(nx.clustering(gp).values())\n",
    "    hist,_ = np.histogram(clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "    sample_pred_clus.append(hist)     \n",
    "    \n",
    "mmd_dist_clus = mmd.compute_mmd(sample_ref_clus, sample_pred_clus, kernel=mmd.gaussian_tv, sigma=1.0 / 10)#, distance_scaling=bins)\n",
    "mmd_dist_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_helper import degree_stats, clustering_stats, orbit_stats_all, eval_fraction_unique, eval_fraction_unique_non_isomorphic_valid, spectral_stats\n",
    "\n",
    "graph_ref_list = []\n",
    "for jj in range(len(test_set)):\n",
    "    laplacian_matrix = np.array(dataset[jj][3].cpu())[:dataset[jj][4],:dataset[jj][4]]\n",
    "    Aori = np.copy(laplacian_matrix)\n",
    "    np.fill_diagonal(Aori,0)\n",
    "    Aori= Aori*(-1)\n",
    "    graph_ref_list.append(nx.from_numpy_array(Aori)) \n",
    "    \n",
    "degree = degree_stats( graph_ref_list,graph_pred_list_remove_empty, compute_emd=False)\n",
    "cluster = clustering_stats( graph_ref_list,graph_pred_list_remove_empty, compute_emd=False)\n",
    "orbit = orbit_stats_all(graph_ref_list_2, graph_ref_list, compute_emd=False)\n",
    "unique,novel,_ = eval_fraction_unique_non_isomorphic_valid(graph_pred_list_remove_empty,graph_ref_list)\n",
    "spectral = spectral_stats(graph_ref_list, graph_pred_list_remove_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3abdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeca0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b151ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#unique and novel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ddc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:graph_diffusion]",
   "language": "python",
   "name": "conda-env-graph_diffusion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
